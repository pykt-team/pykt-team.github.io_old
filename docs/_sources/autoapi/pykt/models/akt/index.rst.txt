:py:mod:`pykt.models.akt`
=========================

.. py:module:: pykt.models.akt


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   pykt.models.akt.Dim
   pykt.models.akt.AKT
   pykt.models.akt.Architecture
   pykt.models.akt.TransformerLayer
   pykt.models.akt.MultiHeadAttention
   pykt.models.akt.LearnablePositionalEmbedding
   pykt.models.akt.CosinePositionalEmbedding



Functions
~~~~~~~~~

.. autoapisummary::

   pykt.models.akt.attention



Attributes
~~~~~~~~~~

.. autoapisummary::

   pykt.models.akt.device


.. py:data:: device
   

   

.. py:class:: Dim

   Bases: :py:obj:`enum.IntEnum`

   Enum where members are also (and must be) ints

   .. py:attribute:: batch
      :annotation: = 0

      

   .. py:attribute:: seq
      :annotation: = 1

      

   .. py:attribute:: feature
      :annotation: = 2

      


.. py:class:: AKT(n_question, n_pid, d_model, n_blocks, dropout, d_ff=256, kq_same=1, final_fc_dim=512, num_attn_heads=8, separate_qa=False, l2=1e-05, emb_type='qid', emb_path='', pretrain_dim=768)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: reset(self)


   .. py:method:: base_emb(self, q_data, target)


   .. py:method:: forward(self, q_data, target, pid_data=None, qtest=False)



.. py:class:: Architecture(n_question, n_blocks, d_model, d_feature, d_ff, n_heads, dropout, kq_same, model_type)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(self, q_embed_data, qa_embed_data)



.. py:class:: TransformerLayer(d_model, d_feature, d_ff, n_heads, dropout, kq_same)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(self, mask, query, key, values, apply_pos=True)

      Input:
          block : object of type BasicBlock(nn.Module). It contains masked_attn_head objects which is of type MultiHeadAttention(nn.Module).
          mask : 0 means, it can peek only past values. 1 means, block can peek only current and pas values
          query : Query. In transformer paper it is the input for both encoder and decoder
          key : Keys. In transformer paper it is the input for both encoder and decoder
          Values. In transformer paper it is the input for encoder and  encoded output for decoder (in masked attention part)

      Output:
          query: Input gets changed over the layer and returned.




.. py:class:: MultiHeadAttention(d_model, d_feature, n_heads, dropout, kq_same, bias=True)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: _reset_parameters(self)


   .. py:method:: forward(self, q, k, v, mask, zero_pad)



.. py:function:: attention(q, k, v, d_k, mask, dropout, zero_pad, gamma=None)

   This is called by Multi-head atention object to find the values.


.. py:class:: LearnablePositionalEmbedding(d_model, max_len=512)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(self, x)



.. py:class:: CosinePositionalEmbedding(d_model, max_len=512)

   Bases: :py:obj:`torch.nn.Module`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super(Model, self).__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward(self, x)



